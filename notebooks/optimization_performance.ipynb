{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport d'optimisation des performances - API Credit Scoring\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Ce notebook documente l'analyse de performance et l'optimisation du pipeline de prédiction de l'API de scoring crédit. \n",
    "\n",
    "**Architecture actuelle :**\n",
    "- Modèle LightGBM (462 estimateurs, max_depth=3, 10 features, fichier pickle ~465 KB)\n",
    "- Pipeline de prédiction : Pydantic → `pd.DataFrame` → réordonnancement colonnes → `predict_proba`\n",
    "- Problème identifié : utilisation de pandas pour une prédiction ligne par ligne (overhead significatif)\n",
    "\n",
    "**Approche :**\n",
    "1. Profilage détaillé du pipeline actuel\n",
    "2. Optimisation 1 : remplacement de pandas par numpy\n",
    "3. Optimisation 2 : conversion ONNX Runtime\n",
    "4. Validation de la précision et benchmarks comparatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "MODEL_PATH = PROJECT_ROOT / \"results\" / \"lightgbm_optimized.pkl\"\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"dataset_top10_features_data.csv\"\n",
    "LOG_PATH = PROJECT_ROOT / \"logs\" / \"predictions.jsonl\"\n",
    "ONNX_OUTPUT_PATH = PROJECT_ROOT / \"results\" / \"lightgbm_optimized.onnx\"\n",
    "\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "FEATURE_ORDER = list(model.feature_name_)\n",
    "print(f\"Modèle chargé : {model.__class__.__name__}\")\n",
    "print(f\"Nombre d'estimateurs : {model.n_estimators}\")\n",
    "print(f\"Profondeur max : {model.max_depth}\")\n",
    "print(f\"Features ({len(FEATURE_ORDER)}) : {FEATURE_ORDER}\")\n",
    "print(f\"Taille du fichier pickle : {MODEL_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse des données de monitoring\n",
    "\n",
    "Avant d'optimiser, analysons les données de latence collectées en production pour établir une baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "with open(LOG_PATH) as f:\n",
    "    for line in f:\n",
    "        logs.append(json.loads(line))\n",
    "\n",
    "df_logs = pd.DataFrame(logs)\n",
    "successful = df_logs[df_logs[\"status_code\"] == 200]\n",
    "\n",
    "print(f\"Nombre total de prédictions loguées : {len(df_logs)}\")\n",
    "print(f\"Prédictions réussies (200) : {len(successful)}\")\n",
    "print(f\"\\nDistribution de la latence (duration_ms) :\")\n",
    "print(successful[\"duration_ms\"].describe().round(2))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(successful[\"duration_ms\"], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"Latence (ms)\")\n",
    "axes[0].set_ylabel(\"Fréquence\")\n",
    "axes[0].set_title(\"Distribution de la latence des prédictions\")\n",
    "axes[0].axvline(successful[\"duration_ms\"].median(), color=\"red\", linestyle=\"--\", label=f\"Médiane: {successful['duration_ms'].median():.1f} ms\")\n",
    "axes[0].legend()\n",
    "\n",
    "decision_counts = successful[\"credit_decision\"].value_counts()\n",
    "axes[1].bar(decision_counts.index, decision_counts.values, color=[\"green\", \"red\"])\n",
    "axes[1].set_ylabel(\"Nombre\")\n",
    "axes[1].set_title(\"Distribution des décisions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Profilage avec cProfile et timeit\n",
    "\n",
    "Micro-benchmark de chaque étape du pipeline de prédiction pour identifier les goulots d'étranglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CreditFeatures(BaseModel):\n",
    "    EXT_SOURCES_MEAN: float = Field(ge=0.0, le=1.0)\n",
    "    CREDIT_TERM: float = Field(ge=0.0, le=1.0)\n",
    "    EXT_SOURCE_3: float = Field(ge=0.0, le=1.0)\n",
    "    GOODS_PRICE_CREDIT_PERCENT: float = Field(ge=0.0, le=1.5)\n",
    "    INSTAL_AMT_PAYMENT_sum: float = Field(ge=0.0, le=1e8)\n",
    "    AMT_ANNUITY: float = Field(gt=0.0, le=1e6)\n",
    "    POS_CNT_INSTALMENT_FUTURE_mean: float = Field(ge=0.0, le=200.0)\n",
    "    DAYS_BIRTH: int = Field(lt=0, ge=-30000)\n",
    "    EXT_SOURCES_WEIGHTED: float = Field(ge=0.0, le=3.0)\n",
    "    EXT_SOURCE_2: float = Field(ge=0.0, le=1.0)\n",
    "\n",
    "SAMPLE_INPUT = {\n",
    "    \"EXT_SOURCES_MEAN\": 0.524,\n",
    "    \"CREDIT_TERM\": 0.05,\n",
    "    \"EXT_SOURCE_3\": 0.535,\n",
    "    \"GOODS_PRICE_CREDIT_PERCENT\": 0.9,\n",
    "    \"INSTAL_AMT_PAYMENT_sum\": 318619.5,\n",
    "    \"AMT_ANNUITY\": 24903.0,\n",
    "    \"POS_CNT_INSTALMENT_FUTURE_mean\": 6.95,\n",
    "    \"DAYS_BIRTH\": -15750,\n",
    "    \"EXT_SOURCES_WEIGHTED\": 1.5,\n",
    "    \"EXT_SOURCE_2\": 0.566,\n",
    "}\n",
    "\n",
    "N_ITERATIONS = 10_000\n",
    "\n",
    "def bench(fn, n=N_ITERATIONS):\n",
    "    \"\"\"Retourne le temps moyen en microsecondes.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n):\n",
    "        fn()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return (elapsed / n) * 1e6  # µs\n",
    "\n",
    "# Étape 1 : Validation Pydantic\n",
    "t_pydantic = bench(lambda: CreditFeatures(**SAMPLE_INPUT))\n",
    "\n",
    "# Étape 2 : Création DataFrame\n",
    "validated = CreditFeatures(**SAMPLE_INPUT)\n",
    "t_dataframe = bench(lambda: pd.DataFrame([validated.model_dump()]))\n",
    "\n",
    "# Étape 3 : Réordonnancement colonnes\n",
    "df_single = pd.DataFrame([validated.model_dump()])\n",
    "t_reorder = bench(lambda: df_single[model.feature_name_])\n",
    "\n",
    "# Étape 4 : predict_proba\n",
    "df_ready = df_single[model.feature_name_]\n",
    "t_predict = bench(lambda: model.predict_proba(df_ready))\n",
    "\n",
    "# Pipeline complet (actuel)\n",
    "def current_pipeline():\n",
    "    features = CreditFeatures(**SAMPLE_INPUT)\n",
    "    df = pd.DataFrame([features.model_dump()])\n",
    "    df = df[model.feature_name_]\n",
    "    return model.predict_proba(df)[0, 1]\n",
    "\n",
    "t_total = bench(current_pipeline)\n",
    "\n",
    "stages = {\n",
    "    \"Pydantic validation\": t_pydantic,\n",
    "    \"DataFrame creation\": t_dataframe,\n",
    "    \"Column reorder\": t_reorder,\n",
    "    \"predict_proba\": t_predict,\n",
    "}\n",
    "\n",
    "print(f\"{'\\u00c9tape':<25} {'Temps moyen (µs)':>15}  {'% du total':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for name, t in stages.items():\n",
    "    print(f\"{name:<25} {t:>15.1f}  {t/t_total*100:>9.1f}%\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Pipeline complet':<25} {t_total:>15.1f}  {'100.0%':>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cProfile du pipeline complet\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "for _ in range(1_000):\n",
    "    current_pipeline()\n",
    "profiler.disable()\n",
    "\n",
    "stream = io.StringIO()\n",
    "stats = pstats.Stats(profiler, stream=stream).sort_stats(\"cumulative\")\n",
    "stats.print_stats(20)\n",
    "print(stream.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique en barres empilées\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "colors = [\"#2196F3\", \"#FF9800\", \"#F44336\", \"#4CAF50\"]\n",
    "bottom = 0\n",
    "for (name, t), color in zip(stages.items(), colors):\n",
    "    ax.barh(\"Pipeline actuel\", t, left=bottom, label=name, color=color)\n",
    "    bottom += t\n",
    "\n",
    "ax.set_xlabel(\"Temps (µs)\")\n",
    "ax.set_title(\"Décomposition du temps de prédiction par étape\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimisation 1 : Tableaux Numpy\n",
    "\n",
    "Remplacement de `pd.DataFrame` par un tableau numpy ordonné. Élimine complètement l'overhead de pandas pour les prédictions unitaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_pipeline():\n",
    "    features = CreditFeatures(**SAMPLE_INPUT)\n",
    "    data = features.model_dump()\n",
    "    row = np.array([[data[f] for f in FEATURE_ORDER]])\n",
    "    return model.predict_proba(row)[0, 1]\n",
    "\n",
    "# Vérification de cohérence\n",
    "p_current = current_pipeline()\n",
    "p_numpy = numpy_pipeline()\n",
    "print(f\"Probabilité pipeline actuel : {p_current:.10f}\")\n",
    "print(f\"Probabilité pipeline numpy  : {p_numpy:.10f}\")\n",
    "print(f\"Différence absolue           : {abs(p_current - p_numpy):.2e}\")\n",
    "assert p_current == p_numpy, \"Les résultats doivent être identiques !\"\n",
    "print(\"\\n✅ Résultats identiques\")\n",
    "\n",
    "# Benchmark\n",
    "t_numpy_total = bench(numpy_pipeline)\n",
    "\n",
    "# Décomposition\n",
    "t_np_array = bench(lambda: np.array([[validated.model_dump()[f] for f in FEATURE_ORDER]]))\n",
    "np_row = np.array([[validated.model_dump()[f] for f in FEATURE_ORDER]])\n",
    "t_np_predict = bench(lambda: model.predict_proba(np_row))\n",
    "\n",
    "print(f\"\\nPipeline numpy : {t_numpy_total:.1f} µs\")\n",
    "print(f\"  - Création array numpy : {t_np_array:.1f} µs\")\n",
    "print(f\"  - predict_proba        : {t_np_predict:.1f} µs\")\n",
    "print(f\"\\nSpeedup vs pipeline actuel : {t_total / t_numpy_total:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimisation 2 : ONNX Runtime\n",
    "\n",
    "Conversion du modèle LightGBM en format ONNX et inférence via ONNX Runtime pour une performance maximale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import onnxmltools\nfrom onnxmltools.convert.common.data_types import FloatTensorType\nimport onnxruntime as ort\n\n# Conversion LightGBM -> ONNX\ninitial_type = [(\"features\", FloatTensorType([None, len(FEATURE_ORDER)]))]\nonnx_model = onnxmltools.convert_lightgbm(\n    model,\n    initial_types=initial_type,\n    name=\"CreditScoringLightGBM\",\n)\n\n# Sauvegarde\nONNX_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\nwith open(ONNX_OUTPUT_PATH, \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nonnx_size_kb = ONNX_OUTPUT_PATH.stat().st_size / 1024\npkl_size_kb = MODEL_PATH.stat().st_size / 1024\nprint(f\"Modèle ONNX sauvegardé : {ONNX_OUTPUT_PATH}\")\nprint(f\"Taille ONNX  : {onnx_size_kb:.1f} KB\")\nprint(f\"Taille pickle : {pkl_size_kb:.1f} KB\")\nprint(f\"Ratio : {onnx_size_kb / pkl_size_kb:.2f}x\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la session ONNX Runtime\n",
    "session = ort.InferenceSession(str(ONNX_OUTPUT_PATH))\n",
    "\n",
    "print(\"Inputs:\")\n",
    "for inp in session.get_inputs():\n",
    "    print(f\"  {inp.name}: shape={inp.shape}, type={inp.type}\")\n",
    "print(\"Outputs:\")\n",
    "for out in session.get_outputs():\n",
    "    print(f\"  {out.name}: shape={out.shape}, type={out.type}\")\n",
    "\n",
    "# Pipeline ONNX\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "def onnx_pipeline():\n",
    "    features = CreditFeatures(**SAMPLE_INPUT)\n",
    "    data = features.model_dump()\n",
    "    row = np.array([[data[f] for f in FEATURE_ORDER]], dtype=np.float32)\n",
    "    result = session.run(None, {input_name: row})\n",
    "    return result[1][0][1]  # probabilities -> row 0 -> class 1\n",
    "\n",
    "# Vérification\n",
    "p_onnx = onnx_pipeline()\n",
    "print(f\"\\nProbabilité pipeline actuel : {p_current:.10f}\")\n",
    "print(f\"Probabilité pipeline ONNX   : {float(p_onnx):.10f}\")\n",
    "print(f\"Différence absolue           : {abs(p_current - float(p_onnx)):.2e}\")\n",
    "\n",
    "# Benchmark\n",
    "t_onnx_total = bench(onnx_pipeline)\n",
    "\n",
    "# Décomposition\n",
    "t_onnx_array = bench(lambda: np.array([[validated.model_dump()[f] for f in FEATURE_ORDER]], dtype=np.float32))\n",
    "onnx_row = np.array([[validated.model_dump()[f] for f in FEATURE_ORDER]], dtype=np.float32)\n",
    "t_onnx_infer = bench(lambda: session.run(None, {input_name: onnx_row}))\n",
    "\n",
    "print(f\"\\nPipeline ONNX : {t_onnx_total:.1f} µs\")\n",
    "print(f\"  - Création array float32 : {t_onnx_array:.1f} µs\")\n",
    "print(f\"  - Inférence ONNX Runtime : {t_onnx_infer:.1f} µs\")\n",
    "print(f\"\\nSpeedup vs pipeline actuel : {t_total / t_onnx_total:.2f}x\")\n",
    "print(f\"Speedup vs pipeline numpy  : {t_numpy_total / t_onnx_total:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation de la précision\n",
    "\n",
    "Comparaison des probabilités sur 500 échantillons et calcul de l'AUC sur le dataset complet pour garantir l'absence de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Charger le dataset complet\n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "X = df_full[FEATURE_ORDER]\n",
    "y = df_full[\"TARGET\"]\n",
    "\n",
    "print(f\"Dataset : {len(df_full)} lignes, {len(FEATURE_ORDER)} features\")\n",
    "\n",
    "# Prédictions sur 500 échantillons\n",
    "np.random.seed(42)\n",
    "idx_sample = np.random.choice(len(X), 500, replace=False)\n",
    "X_sample = X.iloc[idx_sample]\n",
    "\n",
    "# Original (pandas + LightGBM)\n",
    "proba_original = model.predict_proba(X_sample)[:, 1]\n",
    "\n",
    "# Numpy + LightGBM\n",
    "X_np = X_sample.values\n",
    "proba_numpy = model.predict_proba(X_np)[:, 1]\n",
    "\n",
    "# ONNX Runtime\n",
    "X_onnx = X_sample.values.astype(np.float32)\n",
    "result_onnx = session.run(None, {input_name: X_onnx})\n",
    "proba_onnx = np.array([r[1] for r in result_onnx[1]])\n",
    "\n",
    "# Comparaison original vs numpy\n",
    "diff_numpy = np.abs(proba_original - proba_numpy)\n",
    "print(f\"\\n--- Original vs Numpy ---\")\n",
    "print(f\"Diff max  : {diff_numpy.max():.2e}\")\n",
    "print(f\"Diff mean : {diff_numpy.mean():.2e}\")\n",
    "assert np.allclose(proba_original, proba_numpy, atol=0), \"Les résultats numpy doivent être identiques\"\n",
    "print(\"✅ Résultats identiques\")\n",
    "\n",
    "# Comparaison original vs ONNX\n",
    "diff_onnx = np.abs(proba_original - proba_onnx)\n",
    "print(f\"\\n--- Original vs ONNX ---\")\n",
    "print(f\"Diff max  : {diff_onnx.max():.2e}\")\n",
    "print(f\"Diff mean : {diff_onnx.mean():.2e}\")\n",
    "print(f\"Diff < 1e-6 : {(diff_onnx < 1e-6).sum()} / {len(diff_onnx)} ({(diff_onnx < 1e-6).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC sur le dataset complet\n",
    "proba_full_original = model.predict_proba(X)[:, 1]\n",
    "proba_full_onnx = session.run(None, {input_name: X.values.astype(np.float32)})\n",
    "proba_full_onnx = np.array([r[1] for r in proba_full_onnx[1]])\n",
    "\n",
    "auc_original = roc_auc_score(y, proba_full_original)\n",
    "auc_onnx = roc_auc_score(y, proba_full_onnx)\n",
    "\n",
    "print(f\"AUC original (LightGBM) : {auc_original:.6f}\")\n",
    "print(f\"AUC ONNX Runtime        : {auc_onnx:.6f}\")\n",
    "print(f\"Différence AUC           : {abs(auc_original - auc_onnx):.2e}\")\n",
    "\n",
    "# Changements de prédiction au seuil 0.10\n",
    "THRESHOLD = 0.10\n",
    "pred_original = (proba_full_original >= THRESHOLD).astype(int)\n",
    "pred_onnx = (proba_full_onnx >= THRESHOLD).astype(int)\n",
    "n_changes = (pred_original != pred_onnx).sum()\n",
    "\n",
    "print(f\"\\nSeuil de décision : {THRESHOLD}\")\n",
    "print(f\"Changements de prédiction : {n_changes} / {len(pred_original)} ({n_changes/len(pred_original)*100:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tableau comparatif et graphiques\n",
    "\n",
    "Synthèse des résultats de benchmark pour les 3 approches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Approche\": [\"Pandas + LightGBM\", \"Numpy + LightGBM\", \"Numpy + ONNX Runtime\"],\n",
    "    \"Latence (µs)\": [round(t_total, 1), round(t_numpy_total, 1), round(t_onnx_total, 1)],\n",
    "    \"Speedup\": [1.0, round(t_total / t_numpy_total, 2), round(t_total / t_onnx_total, 2)],\n",
    "    \"Taille modèle (KB)\": [round(pkl_size_kb, 1), round(pkl_size_kb, 1), round(onnx_size_kb, 1)],\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    "approaches = results[\"Approche\"]\n",
    "latencies = results[\"Latence (µs)\"]\n",
    "speedups = results[\"Speedup\"]\n",
    "\n",
    "bars = axes[0].bar(approaches, latencies, color=colors, edgecolor=\"black\")\n",
    "axes[0].set_ylabel(\"Latence (µs)\")\n",
    "axes[0].set_title(\"Latence par approche\")\n",
    "for bar, val in zip(bars, latencies):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                 f\"{val} µs\", ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=15)\n",
    "\n",
    "bars2 = axes[1].bar(approaches, speedups, color=colors, edgecolor=\"black\")\n",
    "axes[1].set_ylabel(\"Facteur d'accélération\")\n",
    "axes[1].set_title(\"Speedup vs pipeline actuel\")\n",
    "axes[1].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "for bar, val in zip(bars2, speedups):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                 f\"{val}x\", ha=\"center\", fontweight=\"bold\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmark du temps de réponse API\n",
    "\n",
    "Mesure du temps de réponse HTTP complet (aller-retour) avec les 3 approches via `httpx.TestClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import importlib\n",
    "import httpx\n",
    "\n",
    "N_API_ITERATIONS = 1_000\n",
    "\n",
    "# --- Approche 1 : Pipeline actuel (Pandas + LightGBM) ---\n",
    "# Simuler via monkeypatch pour tester les 3 variantes\n",
    "\n",
    "def bench_api(predict_fn_code, setup_code, label):\n",
    "    \"\"\"Benchmark a prediction approach via FastAPI TestClient.\"\"\"\n",
    "    # Create a temporary app module for benchmarking\n",
    "    exec_globals = {}\n",
    "    exec(setup_code, exec_globals)\n",
    "    exec(predict_fn_code, exec_globals)\n",
    "    return exec_globals\n",
    "\n",
    "# Approche plus simple : benchmark direct des fonctions\n",
    "print(\"Benchmark du temps de réponse API complet (TestClient)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# On crée une mini-app pour chaque approche\n",
    "from fastapi import FastAPI\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "# Approche 1 : Pandas + LightGBM (actuel)\n",
    "app_current = FastAPI()\n",
    "\n",
    "@app_current.post(\"/predict\")\n",
    "def predict_current(features: CreditFeatures):\n",
    "    df = pd.DataFrame([features.model_dump()])\n",
    "    df = df[model.feature_name_]\n",
    "    probability = model.predict_proba(df)[0, 1]\n",
    "    return {\"probability_default\": round(float(probability), 6)}\n",
    "\n",
    "# Approche 2 : Numpy + LightGBM\n",
    "app_numpy = FastAPI()\n",
    "\n",
    "@app_numpy.post(\"/predict\")\n",
    "def predict_numpy(features: CreditFeatures):\n",
    "    data = features.model_dump()\n",
    "    row = np.array([[data[f] for f in FEATURE_ORDER]])\n",
    "    probability = model.predict_proba(row)[0, 1]\n",
    "    return {\"probability_default\": round(float(probability), 6)}\n",
    "\n",
    "# Approche 3 : Numpy + ONNX Runtime\n",
    "app_onnx = FastAPI()\n",
    "\n",
    "@app_onnx.post(\"/predict\")\n",
    "def predict_onnx(features: CreditFeatures):\n",
    "    data = features.model_dump()\n",
    "    row = np.array([[data[f] for f in FEATURE_ORDER]], dtype=np.float32)\n",
    "    result = session.run(None, {input_name: row})\n",
    "    probability = result[1][0][1]\n",
    "    return {\"probability_default\": round(float(probability), 6)}\n",
    "\n",
    "api_results = {}\n",
    "for label, test_app in [(\"Pandas + LightGBM\", app_current),\n",
    "                         (\"Numpy + LightGBM\", app_numpy),\n",
    "                         (\"Numpy + ONNX Runtime\", app_onnx)]:\n",
    "    with TestClient(test_app) as client:\n",
    "        # Warmup\n",
    "        for _ in range(100):\n",
    "            client.post(\"/predict\", json=SAMPLE_INPUT)\n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(N_API_ITERATIONS):\n",
    "            client.post(\"/predict\", json=SAMPLE_INPUT)\n",
    "        elapsed = time.perf_counter() - start\n",
    "    avg_ms = (elapsed / N_API_ITERATIONS) * 1000\n",
    "    api_results[label] = avg_ms\n",
    "    print(f\"{label:<25} : {avg_ms:.2f} ms/req\")\n",
    "\n",
    "print(f\"\\nSpeedup ONNX vs actuel : {api_results['Pandas + LightGBM'] / api_results['Numpy + ONNX Runtime']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse des ressources\n",
    "\n",
    "Comparaison des empreintes mémoire, tailles de fichiers, et impact sur l'image Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Tailles des fichiers modèle\n",
    "print(\"=== Tailles des fichiers modèle ===\")\n",
    "print(f\"LightGBM pickle : {MODEL_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"ONNX            : {ONNX_OUTPUT_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"Ratio ONNX/pickle : {ONNX_OUTPUT_PATH.stat().st_size / MODEL_PATH.stat().st_size:.2f}x\")\n",
    "\n",
    "# Taille mémoire des objets chargés\n",
    "print(f\"\\n=== Empreinte mémoire (approx) ===\")\n",
    "print(f\"Objet LightGBM en mémoire : {sys.getsizeof(model) / 1024:.1f} KB\")\n",
    "print(f\"Session ONNX Runtime     : {sys.getsizeof(session) / 1024:.1f} KB\")\n",
    "\n",
    "# Impact Docker\n",
    "print(f\"\\n=== Impact sur l'image Docker (estimé) ===\")\n",
    "print(f\"Dépendances retirées :\")\n",
    "print(f\"  - scikit-learn : ~25 MB\")\n",
    "print(f\"  - lightgbm     : ~5 MB\")\n",
    "print(f\"  - pandas       : ~70 MB\")\n",
    "print(f\"  Total retiré  : ~100 MB\")\n",
    "print(f\"\\nDépendance ajoutée :\")\n",
    "print(f\"  - onnxruntime  : ~50 MB\")\n",
    "print(f\"\\nRéduction nette estimée : ~50 MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Synthèse\n",
    "\n",
    "Résumé des résultats clés de l'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"       SYNTHÈSE DE L'OPTIMISATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNombre d'itérations benchmark : {N_ITERATIONS:,}\")\n",
    "print(f\"Nombre d'itérations API       : {N_API_ITERATIONS:,}\")\n",
    "print(f\"\\n--- Pipeline de prédiction ---\")\n",
    "print(f\"Latence initiale (Pandas+LGB)   : {t_total:.1f} µs\")\n",
    "print(f\"Latence numpy (Numpy+LGB)       : {t_numpy_total:.1f} µs ({t_total/t_numpy_total:.2f}x)\")\n",
    "print(f\"Latence finale (Numpy+ONNX)     : {t_onnx_total:.1f} µs ({t_total/t_onnx_total:.2f}x)\")\n",
    "print(f\"\\n--- Validation ---\")\n",
    "print(f\"AUC original  : {auc_original:.6f}\")\n",
    "print(f\"AUC ONNX      : {auc_onnx:.6f}\")\n",
    "print(f\"Changements de prédiction (seuil {THRESHOLD}) : {n_changes}\")\n",
    "print(f\"\\n--- Taille ---\")\n",
    "print(f\"Modèle pickle : {pkl_size_kb:.1f} KB\")\n",
    "print(f\"Modèle ONNX   : {onnx_size_kb:.1f} KB\")\n",
    "print(f\"Réduction image Docker estimée : ~50 MB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Recommandations\n",
    "\n",
    "### Configuration retenue : Numpy + ONNX Runtime\n",
    "\n",
    "**Justification :**\n",
    "- Élimination complète de pandas du chemin critique de prédiction\n",
    "- ONNX Runtime optimisé pour l'inférence en production (graph optimization, thread pooling)\n",
    "- Réduction de la taille de l'image Docker (~50 MB de dépendances en moins)\n",
    "- Aucune régression sur l'AUC\n",
    "\n",
    "### Points de vigilance\n",
    "\n",
    "1. **Précision float32** : ONNX utilise float32 par défaut (vs float64 pour LightGBM natif). Les différences sont négligeables (~1e-7) mais pourraient théoriquement affecter des prédictions proches du seuil de décision (0.10). Surveiller via le monitoring existant.\n",
    "\n",
    "2. **Ordre des features** : L'ordre des features est désormais hardcodé dans `FEATURE_ORDER` (dans `api/app.py`). Tout changement dans les features du modèle nécessite une mise à jour synchronisée.\n",
    "\n",
    "3. **Versionnage du modèle** : Le fichier `.onnx` doit être versionné avec le code. Inclure un hash de vérification si nécessaire.\n",
    "\n",
    "### Pistes d'amélioration futures\n",
    "\n",
    "- **Quantification INT8** : Réduction supplémentaire de la taille du modèle et potentiel gain de performance\n",
    "- **Batching** : Pour des scénarios de prédiction en lot, ONNX Runtime supporte nativement le batch processing\n",
    "- **ONNX Runtime Mobile** : Pour un déploiement edge/mobile si nécessaire\n",
    "- **Graph optimization level** : Tester `ort.SessionOptions()` avec `graph_optimization_level = ORT_ENABLE_ALL`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}